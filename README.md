<h1>100 Days of ML Code</h1>
<hr>
<h2>Day 0: 14th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Basic Setup of environment and enrollment in Udacity's Machine Learning intro course and completed Introduction videos.</p>
<h4>Thoughts:</h4>
<p>The course is amazing and excited about 100 days of ML Code.</p>
<h4>Link of Work:</h4>
<p>Just enrolled in this course: https://in.udacity.com/course/intro-to-machine-learning--ud120-india</p>
<hr>

<h2>Day 1: 15th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt how exactly gradient descent works with mathematical derivations and also learnt Naive bayes for classification and implemented using sklearn</p>
<h4>Thoughts:</h4>
<p>Gone crazy in mathematical proofs for gradient descent but very important to understand why we are doing everything and next sklearn is very easy to use.</p>
<hr>

<h2>Day 2: 16th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt and solved problems on Bayes rule and learnt it's applications. Started developing the mini project on naive bayes.</p>
<h4>Thoughts:</h4>
<p>Once J.K.Rowling published a book name under false author name but the computers were able to identify that it was her and it's exciting to know that it can be done using naive bayes.</p>
<hr>

<h2>Day 3: 17th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt about McCulloh pits neuron, their advantages and disadvantages and moved on to perceptron and also learnt perceptron algorithm</p>
<h4>Thoughts:</h4>
<p>Thought that I could the complete project started on day:2 but I couldn't as the dataset was huge, so learnt moved onto deep learning and learnt the perceptron stuff.</p>
<hr>

<h2>Day 4: 18th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Set up the python 2.7 environment ans finally completed the mini project on naive bayes and acheived an accuracy of 97.32%</p>
<h4>Thoughts:</h4>
<p>The dataset was huge and it took much time to download and extract but it is good to do some mini projects as they clear the concepts.</p>
<hr>

<h2>Day 5: 19th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt about linearly saperable boolean functions, Sigmoid neuron, the typical machine learning setup.</p>
<h4>Thoughts:</h4>
<p>Understood the need for using sigmoid neuron, and the basic machine learning things needed to solve any problem.</p>
<hr>

<h2>Day 6: 20th August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt about feed forward forward neural networks and how to learn the parameters the feed forward neural networks.</p>
<h4>Thoughts:</h4>
<p>It's interesting to learn how to apply gradient descent for multilayer neural networks and the proofs.</p>
<hr>

<h2>Day 7: 21st August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt about Entropy, Cross Entropy and their derivations and understood which loss function to choose depending on outputs.</p>
<h4>Thoughts:</h4>
<p>The deeper you go into deep learning the more it is confusing. It took so much time to understand the difference between entropy and cross entropy.</p>
<hr>


<h2>Day 8: 22nd August 2018</h2>
<h4>Todays Progress:</h4>
<p>Learnt the backpropagation intuition and the derivations of the gradent with respect to the output units.</p>
<h4>Thoughts:</h4>
<p>Really exciting to know how exaclty backpropagation works with mathematical derivations, but just completed on output layer need to still calculate with respect to hidden layers, weights and biases</p>
<hr>
